{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5b7e27-958e-4b68-8cdf-d75dae076890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import transformers # pre-trained transformer models (BERT)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "def setup_nltk():\n",
    "    \"\"\"Download required NLTK data packages.\"\"\"\n",
    "    try:\n",
    "        required_packages = ['punkt', 'stopwords']\n",
    "        for package in required_packages:\n",
    "            try:\n",
    "                nltk.data.find(f'tokenizers/{package}')\n",
    "            except LookupError:\n",
    "                print(f\"Downloading {package}...\")\n",
    "                nltk.download(package, quiet=True)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up NLTK: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4003dd-c7ea-4aae-aa06-ab6420f04dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of data for BERT processing.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            sentence_pairs,\n",
    "            labels=None,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            include_targets=True,\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.sentence_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indexes = self.indexes[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        batch_sentence_pairs = self.sentence_pairs[batch_indexes]\n",
    "\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            batch_sentence_pairs.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[batch_indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258c0d7c-ae70-420a-84e1-ef4f669dca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    \"\"\"Simple word tokenization without relying on NLTK's punkt.\"\"\"\n",
    "    # Basic cleaning\n",
    "    text = text.lower().strip()\n",
    "    # Split on whitespace\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "\n",
    "def calculate_similarity_score(teacher_answer, student_answer, tokenizer, model):\n",
    "    \"\"\"Calculate cosine similarity between teacher's and student's answers using BERT embeddings.\"\"\"\n",
    "    # Tokenize and get embeddings\n",
    "    def get_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Use [CLS] token embedding\n",
    "\n",
    "    teacher_embedding = get_embedding(teacher_answer)\n",
    "    student_embedding = get_embedding(student_answer)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity([teacher_embedding], [student_embedding])[0][0]\n",
    "    return similarity\n",
    "\n",
    "def calculate_length_score(teacher_answer, student_answer):\n",
    "    \"\"\"Simple length comparison between teacher's and student's answers.\"\"\"\n",
    "    teacher_len = len(teacher_answer.split())\n",
    "    student_len = len(student_answer.split())\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if teacher_len == 0:\n",
    "        return 0\n",
    "\n",
    "    # Allow some flexibility in length, but penalize very short answers\n",
    "    length_ratio = min(student_len / teacher_len, 1)\n",
    "    return length_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b844f2-1ceb-4857-bca9-a60c861cd35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_answers(teacher_file, student_file, total_test_marks):\n",
    "    \"\"\"Process answers from Excel files and calculate scores using BERT from Hugging Face.\"\"\"\n",
    "    try:\n",
    "        # Load BERT model and tokenizer\n",
    "        print(\"Loading BERT model...\")\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        # Read Excel files into DataFrames\n",
    "        print(\"Reading Excel files...\")\n",
    "        teacher_df = pd.read_excel(teacher_file)\n",
    "        student_df = pd.read_excel(student_file)\n",
    "\n",
    "        # Validate required columns\n",
    "        if teacher_df.shape[1] < 3:\n",
    "            raise ValueError(\"Teacher Excel file must have at least 3 columns: Question, Answer, Total Marks.\")\n",
    "        if student_df.shape[1] < 2:\n",
    "            raise ValueError(\"Student Excel file must have at least 2 columns: Question, Answer.\")\n",
    "\n",
    "        # Create dictionaries for quick lookup\n",
    "        teacher_dict = {str(row[0]).strip(): str(row[1]).strip() for idx, row in teacher_df.iterrows()}\n",
    "        marks_dict = {str(row[0]).strip(): float(row[2]) for idx, row in teacher_df.iterrows()}\n",
    "        student_dict = {str(row[0]).strip(): str(row[1]).strip() for idx, row in student_df.iterrows()}\n",
    "\n",
    "        # Check if questions match\n",
    "        if set(teacher_dict.keys()) != set(student_dict.keys()):\n",
    "            missing_in_student = set(teacher_dict.keys()) - set(student_dict.keys())\n",
    "            missing_in_teacher = set(student_dict.keys()) - set(teacher_dict.keys())\n",
    "            error_msg = \"\"\n",
    "            if missing_in_student:\n",
    "                error_msg += f\"Questions missing in student file: {missing_in_student}\\n\"\n",
    "            if missing_in_teacher:\n",
    "                error_msg += f\"Questions missing in teacher file: {missing_in_teacher}\\n\"\n",
    "            if error_msg:\n",
    "                raise ValueError(error_msg)\n",
    "\n",
    "        # Initialize total marks and obtained marks\n",
    "        total_marks_allotted = sum(marks_dict.values())\n",
    "        if total_marks_allotted == 0:\n",
    "            raise ValueError(\"Total marks allotted from teacher file is zero.\")\n",
    "\n",
    "        if total_test_marks != total_marks_allotted:\n",
    "            print(f\"Warning: The sum of marks per question ({total_marks_allotted}) does not match the total test marks ({total_test_marks}). Proceeding with normalization.\")\n",
    "\n",
    "        # Initialize results\n",
    "        results = []\n",
    "        total_obtained_marks = 0\n",
    "\n",
    "        print(\"Calculating scores for each question...\")\n",
    "\n",
    "        for question in teacher_dict:\n",
    "            teacher_answer = teacher_dict[question]\n",
    "            student_answer = student_dict[question]\n",
    "            marks_allotted = marks_dict[question]\n",
    "\n",
    "            # Calculate similarity using BERT\n",
    "            similarity_score = calculate_similarity_score(teacher_answer, student_answer, tokenizer, model)\n",
    "            length_score = calculate_length_score(teacher_answer, student_answer)\n",
    "\n",
    "            # Final score for the question\n",
    "            question_score = similarity_score * length_score * marks_allotted\n",
    "\n",
    "            # Accumulate total obtained marks\n",
    "            total_obtained_marks += question_score\n",
    "\n",
    "            # Append individual question results\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"teacher_answer\": teacher_answer,\n",
    "                \"student_answer\": student_answer,\n",
    "                \"similarity_score\": round(similarity_score * 100, 2),  # Percentage\n",
    "                \"length_score\": round(length_score, 2),\n",
    "                \"marks_allotted\": marks_allotted,\n",
    "                \"question_score\": round(question_score, 2)\n",
    "            })\n",
    "\n",
    "        # Normalize total obtained marks to the total test marks\n",
    "        normalization_factor = total_test_marks / total_marks_allotted\n",
    "        final_score = int(total_obtained_marks * normalization_factor)\n",
    "\n",
    "        return {\n",
    "            \"results_per_question\": results,\n",
    "            \"total_obtained_marks\": round(total_obtained_marks, 2),\n",
    "            \"total_marks_allotted\": total_marks_allotted,\n",
    "            \"total_test_marks\": total_test_marks,\n",
    "            \"final_score\": final_score\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495468fa-752a-4a10-b774-83c11ff49221",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting answer evaluation...\")\n",
    "\n",
    "    # Prompt for total marks of the test.\n",
    "    while True:\n",
    "        try:\n",
    "            total_test_marks = float(input(\"Enter the total marks for the test: \"))\n",
    "            if total_test_marks <= 0:  # negative or zero inputs.\n",
    "                print(\"Total marks must be a positive number. Please try again.\")\n",
    "                continue\n",
    "            break\n",
    "        except ValueError:  # non-numeric inputs.\n",
    "            print(\"Invalid input. Please enter a numeric value for total marks.\")\n",
    "            \n",
    "    student_path = 'C:/Users/suury/Desktop/Mini Project Sem5/Interface/student_answers.xlsx'\n",
    "    teacher_path = 'C:/Users/suury/Desktop/Mini Project Sem5/Interface/teacher_answers.xlsx'\n",
    "    \n",
    "    # Process the Excel files.\n",
    "    results = process_answers(teacher_path, student_path, total_test_marks)\n",
    "\n",
    "    if \"error\" in results:\n",
    "        print(f\"\\nError: {results['error']}\")\n",
    "    else:\n",
    "        print(\"\\nAnswer Evaluation Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        for idx, res in enumerate(results[\"results_per_question\"], start=1):\n",
    "            print(f\"Question {idx}: {res['question']}\")\n",
    "            print(f\"Teacher's Answer: {res['teacher_answer']}\")\n",
    "            print(f\"Student's Answer: {res['student_answer']}\")\n",
    "            print(\"\\nScores:\")\n",
    "            # Adjusted the print statements to use the single similarity score.\n",
    "            print(f\"  Similarity Score: {res['similarity_score']}%\")\n",
    "            print(f\"  Length Score: {res['length_score']:.2f}\")\n",
    "            print(f\"  Marks Allotted: {res['marks_allotted']}\")\n",
    "            print(f\"  Question Score: {res['question_score']}/{res['marks_allotted']}\\n\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        print(\"Summary:\")\n",
    "        print(f\"Total Obtained Marks: {results['total_obtained_marks']}/{results['total_marks_allotted']}\")\n",
    "        print(f\"Total Test Marks: {results['total_test_marks']}\")\n",
    "        print(f\"Final Score: {results['final_score']}/{results['total_test_marks']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269e218-e020-499d-a6e1-3859c03328c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
